{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import librosa as lb\n",
    "import librosa.display\n",
    "import scipy\n",
    "import json\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from model_builder import build_example\n",
    "from plotter import plot_history\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "\n",
    "DATA_DIR = \"openmic-2018/\"\n",
    "CATEGORY_COUNT = 8\n",
    "LEARNING_RATE = 0.00001\n",
    "THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenMIC keys: ['MEL', 'Y_true', 'Y_mask', 'sample_key']\n",
      "X has shape: (20000, 128, 430)\n",
      "Y_true has shape: (20000, 20)\n",
      "Y_mask has shape: (20000, 20)\n",
      "sample_key has shape: (20000,)\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "OPENMIC = np.load(os.path.join(DATA_DIR, 'openmic-mel.npz'), allow_pickle=True)\n",
    "print('OpenMIC keys: ' + str(list(OPENMIC.keys())))\n",
    "X, Y_true, Y_mask, sample_key = OPENMIC['MEL'], OPENMIC['Y_true'], OPENMIC['Y_mask'], OPENMIC['sample_key']\n",
    "print('X has shape: ' + str(X.shape))\n",
    "print('Y_true has shape: ' + str(Y_true.shape))\n",
    "print('Y_mask has shape: ' + str(Y_mask.shape))\n",
    "print('sample_key has shape: ' + str(sample_key.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenMIC instruments: {'accordion': 0, 'banjo': 1, 'bass': 2, 'cello': 3, 'clarinet': 4, 'cymbals': 5, 'drums': 6, 'flute': 7, 'guitar': 8, 'mallet_percussion': 9, 'mandolin': 10, 'organ': 11, 'piano': 12, 'saxophone': 13, 'synthesizer': 14, 'trombone': 15, 'trumpet': 16, 'ukulele': 17, 'violin': 18, 'voice': 19}\n"
     ]
    }
   ],
   "source": [
    "# LOAD LABELS\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'class-map.json'), 'r') as f:\n",
    "    INSTRUMENTS = json.load(f)\n",
    "print('OpenMIC instruments: ' + str(INSTRUMENTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train: 15000, # Val: 2500, # Test: 2500\n"
     ]
    }
   ],
   "source": [
    "# SPLIT DATA (TRAIN - TEST - VAL)\n",
    "\n",
    "# CHANGE X TO MEL\n",
    "split_train, split_test, X_train, X_test, Y_true_train, Y_true_test, Y_mask_train, Y_mask_test = train_test_split(sample_key, X, Y_true, Y_mask)\n",
    "split_val, split_test, X_val, X_test, Y_true_val, Y_true_test, Y_mask_val, Y_mask_test = train_test_split(split_test, X_test, Y_true_test, Y_mask_test, test_size=0.5)\n",
    "train_set = np.asarray(set(split_train))\n",
    "test_set = np.asarray(set(split_test))\n",
    "print('# Train: {}, # Val: {}, # Test: {}'.format(len(split_train), len(split_test), len(split_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 128, 430)\n",
      "(2500, 128, 430)\n",
      "accordion\n",
      "TRAIN: 366 true of 1535 (23.84 %)\n",
      "TEST: 66 true of 269 (24.54 %)\n",
      "VALIDATION: 57 true of 267 (21.35 %)\n",
      "banjo\n",
      "TRAIN: 541 true of 1661 (32.57 %)\n",
      "TEST: 99 true of 287 (34.49 %)\n",
      "VALIDATION: 92 true of 270 (34.07 %)\n",
      "bass\n",
      "TRAIN: 400 true of 1417 (28.23 %)\n",
      "TEST: 75 true of 227 (33.04 %)\n",
      "VALIDATION: 74 true of 244 (30.33 %)\n",
      "cello\n",
      "TRAIN: 608 true of 1458 (41.7 %)\n",
      "TEST: 110 true of 255 (43.14 %)\n",
      "VALIDATION: 106 true of 236 (44.92 %)\n",
      "clarinet\n",
      "TRAIN: 400 true of 1790 (22.35 %)\n",
      "TEST: 68 true of 291 (23.37 %)\n",
      "VALIDATION: 65 true of 304 (21.38 %)\n",
      "cymbals\n",
      "TRAIN: 824 true of 1296 (63.58 %)\n",
      "TEST: 156 true of 241 (64.73 %)\n",
      "VALIDATION: 131 true of 198 (66.16 %)\n",
      "drums\n",
      "TRAIN: 835 true of 1332 (62.69 %)\n",
      "TEST: 142 true of 215 (66.05 %)\n",
      "VALIDATION: 129 true of 200 (64.5 %)\n",
      "flute\n",
      "TRAIN: 492 true of 1565 (31.44 %)\n",
      "TEST: 74 true of 249 (29.72 %)\n",
      "VALIDATION: 81 true of 270 (30.0 %)\n",
      "guitar\n",
      "TRAIN: 826 true of 1215 (67.98 %)\n",
      "TEST: 150 true of 214 (70.09 %)\n",
      "VALIDATION: 162 true of 221 (73.3 %)\n",
      "mallet_percussion\n",
      "TRAIN: 533 true of 1348 (39.54 %)\n",
      "TEST: 101 true of 225 (44.89 %)\n",
      "VALIDATION: 99 true of 229 (43.23 %)\n",
      "mandolin\n",
      "TRAIN: 648 true of 1868 (34.69 %)\n",
      "TEST: 103 true of 301 (34.22 %)\n",
      "VALIDATION: 94 true of 295 (31.86 %)\n",
      "organ\n",
      "TRAIN: 452 true of 1438 (31.43 %)\n",
      "TEST: 70 true of 234 (29.91 %)\n",
      "VALIDATION: 81 true of 218 (37.16 %)\n",
      "piano\n",
      "TRAIN: 879 true of 1292 (68.03 %)\n",
      "TEST: 154 true of 220 (70.0 %)\n",
      "VALIDATION: 137 true of 208 (65.87 %)\n",
      "saxophone\n",
      "TRAIN: 846 true of 1750 (48.34 %)\n",
      "TEST: 133 true of 293 (45.39 %)\n",
      "VALIDATION: 156 true of 322 (48.45 %)\n",
      "synthesizer\n",
      "TRAIN: 819 true of 1202 (68.14 %)\n",
      "TEST: 140 true of 199 (70.35 %)\n",
      "VALIDATION: 132 true of 201 (65.67 %)\n",
      "trombone\n",
      "TRAIN: 657 true of 2060 (31.89 %)\n",
      "TEST: 105 true of 348 (30.17 %)\n",
      "VALIDATION: 101 true of 352 (28.69 %)\n",
      "trumpet\n",
      "TRAIN: 849 true of 2200 (38.59 %)\n",
      "TEST: 142 true of 348 (40.8 %)\n",
      "VALIDATION: 155 true of 368 (42.12 %)\n",
      "ukulele\n",
      "TRAIN: 559 true of 1819 (30.73 %)\n",
      "TEST: 95 true of 314 (30.25 %)\n",
      "VALIDATION: 84 true of 292 (28.77 %)\n",
      "violin\n",
      "TRAIN: 881 true of 1535 (57.39 %)\n",
      "TEST: 141 true of 229 (61.57 %)\n",
      "VALIDATION: 151 true of 269 (56.13 %)\n",
      "voice\n",
      "TRAIN: 744 true of 1181 (63.0 %)\n",
      "TEST: 121 true of 191 (63.35 %)\n",
      "VALIDATION: 123 true of 192 (64.06 %)\n"
     ]
    }
   ],
   "source": [
    "# DUPLICATE OF THE MODEL PREPROCESS\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "for instrument in INSTRUMENTS:\n",
    "    \n",
    "    # Map the instrument name to its column number\n",
    "    inst_num = INSTRUMENTS[instrument]\n",
    "\n",
    "    print(instrument)\n",
    "    \n",
    "    # TRAIN\n",
    "    train_inst = Y_mask_train[:, inst_num]\n",
    "    X_train_inst = X_train[train_inst]\n",
    "    X_train_inst = X_train_inst.astype('float16')\n",
    "    shape = X_train_inst.shape\n",
    "    X_train_inst = X_train_inst.reshape(shape[0],1, shape[1], shape[2])\n",
    "    Y_true_train_inst = Y_true_train[train_inst, inst_num] >= THRESHOLD\n",
    "    i = 0\n",
    "    for val in Y_true_train_inst:\n",
    "        i += val\n",
    "        \n",
    "    print('TRAIN: ' + str(i) + ' true of ' + str(len(Y_true_train_inst)) + ' (' + str(round(i / len(Y_true_train_inst ) * 100,2)) + ' %)' )\n",
    "        \n",
    "    \n",
    "    # TEST\n",
    "    test_inst = Y_mask_test[:, inst_num]\n",
    "    X_test_inst = X_test[test_inst]\n",
    "    X_test_inst = X_test_inst.astype('float16')\n",
    "    shape = X_test_inst.shape\n",
    "    X_test_inst = X_test_inst.reshape(shape[0],1, shape[1], shape[2])\n",
    "    Y_true_test_inst = Y_true_test[test_inst, inst_num] >= THRESHOLD\n",
    "    \n",
    "    i = 0\n",
    "    for val in Y_true_test_inst:\n",
    "        i += val\n",
    "        \n",
    "    print('TEST: ' + str(i) + ' true of ' + str(len(Y_true_test_inst)) + ' (' + str(round(i / len(Y_true_test_inst ) * 100,2)) + ' %)' )\n",
    "    \n",
    "    \n",
    "    # VALIDATION\n",
    "    val_inst = Y_mask_val[:, inst_num]\n",
    "    X_val_inst = X_val[val_inst]\n",
    "    X_val_inst = X_val_inst.astype('float16')\n",
    "    shape = X_val_inst.shape\n",
    "    X_val_inst = X_val_inst.reshape(shape[0],1, shape[1], shape[2])\n",
    "    Y_true_val_inst = Y_true_val[val_inst, inst_num] >= THRESHOLD\n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    for val in Y_true_val_inst:\n",
    "        i += val\n",
    "    print('VALIDATION: ' + str(i) + ' true of ' + str(len(Y_true_val_inst)) + ' (' + str(round(i / len(Y_true_val_inst ) * 100,2)) + ' %)' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VALAMI FANCY ADATKIÍRÁS\n",
    "len(Y_true_val_inst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "accordion\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      1.00      0.90      1169\n",
      "        True       1.00      0.31      0.48       366\n",
      "\n",
      "    accuracy                           0.84      1535\n",
      "   macro avg       0.91      0.66      0.69      1535\n",
      "weighted avg       0.87      0.84      0.80      1535\n",
      "\n",
      "False\n",
      "False\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      1.00      0.86       203\n",
      "        True       1.00      0.02      0.03        66\n",
      "\n",
      "    accuracy                           0.76       269\n",
      "   macro avg       0.88      0.51      0.45       269\n",
      "weighted avg       0.82      0.76      0.66       269\n",
      "\n",
      "(269,)\n",
      "(269,)\n",
      "----------------------------------------------------\n",
      "banjo\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.87      1.00      0.93      1120\n",
      "        True       1.00      0.70      0.82       541\n",
      "\n",
      "    accuracy                           0.90      1661\n",
      "   macro avg       0.94      0.85      0.88      1661\n",
      "weighted avg       0.91      0.90      0.90      1661\n",
      "\n",
      "True\n",
      "False\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.98      0.78       188\n",
      "        True       0.00      0.00      0.00        99\n",
      "\n",
      "    accuracy                           0.64       287\n",
      "   macro avg       0.33      0.49      0.39       287\n",
      "weighted avg       0.43      0.64      0.51       287\n",
      "\n",
      "(287,)\n",
      "(287,)\n",
      "----------------------------------------------------\n",
      "bass\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.87      1.00      0.93      1017\n",
      "        True       1.00      0.62      0.76       400\n",
      "\n",
      "    accuracy                           0.89      1417\n",
      "   macro avg       0.93      0.81      0.85      1417\n",
      "weighted avg       0.91      0.89      0.88      1417\n",
      "\n",
      "False\n",
      "False\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.67      0.97      0.79       152\n",
      "        True       0.33      0.03      0.05        75\n",
      "\n",
      "    accuracy                           0.66       227\n",
      "   macro avg       0.50      0.50      0.42       227\n",
      "weighted avg       0.56      0.66      0.55       227\n",
      "\n",
      "(227,)\n",
      "(227,)\n",
      "----------------------------------------------------\n",
      "cello\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.80      0.88       850\n",
      "        True       0.78      0.96      0.86       608\n",
      "\n",
      "    accuracy                           0.87      1458\n",
      "   macro avg       0.87      0.88      0.87      1458\n",
      "weighted avg       0.89      0.87      0.87      1458\n",
      "\n",
      "False\n",
      "False\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.69      0.64      0.67       145\n",
      "        True       0.57      0.63      0.60       110\n",
      "\n",
      "    accuracy                           0.64       255\n",
      "   macro avg       0.63      0.63      0.63       255\n",
      "weighted avg       0.64      0.64      0.64       255\n",
      "\n",
      "(255,)\n",
      "(255,)\n",
      "----------------------------------------------------\n",
      "clarinet\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      1.00      0.91      1390\n",
      "        True       1.00      0.32      0.48       400\n",
      "\n",
      "    accuracy                           0.85      1790\n",
      "   macro avg       0.92      0.66      0.70      1790\n",
      "weighted avg       0.87      0.85      0.82      1790\n",
      "\n",
      "False\n",
      "False\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.77      1.00      0.87       223\n",
      "        True       0.00      0.00      0.00        68\n",
      "\n",
      "    accuracy                           0.77       291\n",
      "   macro avg       0.38      0.50      0.43       291\n",
      "weighted avg       0.59      0.77      0.66       291\n",
      "\n",
      "(291,)\n",
      "(291,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hjani\\Documents\\Conda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "cymbals\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.96      0.95       472\n",
      "        True       0.98      0.97      0.97       824\n",
      "\n",
      "    accuracy                           0.97      1296\n",
      "   macro avg       0.96      0.97      0.96      1296\n",
      "weighted avg       0.97      0.97      0.97      1296\n",
      "\n",
      "True\n",
      "True\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.71      0.74        85\n",
      "        True       0.85      0.89      0.87       156\n",
      "\n",
      "    accuracy                           0.83       241\n",
      "   macro avg       0.81      0.80      0.80       241\n",
      "weighted avg       0.82      0.83      0.82       241\n",
      "\n",
      "(241,)\n",
      "(241,)\n",
      "----------------------------------------------------\n",
      "drums\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.91      0.92       497\n",
      "        True       0.95      0.96      0.95       835\n",
      "\n",
      "    accuracy                           0.94      1332\n",
      "   macro avg       0.94      0.94      0.94      1332\n",
      "weighted avg       0.94      0.94      0.94      1332\n",
      "\n",
      "True\n",
      "True\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.79      0.73      0.76        73\n",
      "        True       0.86      0.90      0.88       142\n",
      "\n",
      "    accuracy                           0.84       215\n",
      "   macro avg       0.83      0.81      0.82       215\n",
      "weighted avg       0.84      0.84      0.84       215\n",
      "\n",
      "(215,)\n",
      "(215,)\n",
      "----------------------------------------------------\n",
      "flute\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      1.00      0.89      1073\n",
      "        True       1.00      0.44      0.61       492\n",
      "\n",
      "    accuracy                           0.82      1565\n",
      "   macro avg       0.90      0.72      0.75      1565\n",
      "weighted avg       0.86      0.82      0.80      1565\n",
      "\n",
      "False\n",
      "False\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.70      1.00      0.83       175\n",
      "        True       0.00      0.00      0.00        74\n",
      "\n",
      "    accuracy                           0.70       249\n",
      "   macro avg       0.35      0.50      0.41       249\n",
      "weighted avg       0.49      0.70      0.58       249\n",
      "\n",
      "(249,)\n",
      "(249,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hjani\\Documents\\Conda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "guitar\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.57      0.73       389\n",
      "        True       0.83      1.00      0.91       826\n",
      "\n",
      "    accuracy                           0.86      1215\n",
      "   macro avg       0.92      0.79      0.82      1215\n",
      "weighted avg       0.89      0.86      0.85      1215\n",
      "\n",
      "True\n",
      "True\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.45      0.08      0.13        64\n",
      "        True       0.71      0.96      0.82       150\n",
      "\n",
      "    accuracy                           0.70       214\n",
      "   macro avg       0.58      0.52      0.47       214\n",
      "weighted avg       0.63      0.70      0.61       214\n",
      "\n",
      "(214,)\n",
      "(214,)\n",
      "----------------------------------------------------\n",
      "mallet_percussion\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.99      0.96       815\n",
      "        True       0.99      0.89      0.94       533\n",
      "\n",
      "    accuracy                           0.95      1348\n",
      "   macro avg       0.96      0.94      0.95      1348\n",
      "weighted avg       0.95      0.95      0.95      1348\n",
      "\n",
      "True\n",
      "False\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.58      0.82      0.68       124\n",
      "        True       0.55      0.27      0.36       101\n",
      "\n",
      "    accuracy                           0.57       225\n",
      "   macro avg       0.57      0.54      0.52       225\n",
      "weighted avg       0.57      0.57      0.54       225\n",
      "\n",
      "(225,)\n",
      "(225,)\n",
      "----------------------------------------------------\n",
      "mandolin\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      1.00      0.89      1220\n",
      "        True       1.00      0.55      0.71       648\n",
      "\n",
      "    accuracy                           0.84      1868\n",
      "   macro avg       0.90      0.77      0.80      1868\n",
      "weighted avg       0.87      0.84      0.83      1868\n",
      "\n",
      "False\n",
      "False\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.98      0.79       198\n",
      "        True       0.00      0.00      0.00       103\n",
      "\n",
      "    accuracy                           0.65       301\n",
      "   macro avg       0.33      0.49      0.39       301\n",
      "weighted avg       0.43      0.65      0.52       301\n",
      "\n",
      "(301,)\n",
      "(301,)\n",
      "----------------------------------------------------\n",
      "organ\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      0.99      0.93       986\n",
      "        True       0.96      0.70      0.81       452\n",
      "\n",
      "    accuracy                           0.90      1438\n",
      "   macro avg       0.92      0.84      0.87      1438\n",
      "weighted avg       0.90      0.90      0.89      1438\n",
      "\n",
      "True\n",
      "False\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.74      0.96      0.84       164\n",
      "        True       0.70      0.23      0.34        70\n",
      "\n",
      "    accuracy                           0.74       234\n",
      "   macro avg       0.72      0.59      0.59       234\n",
      "weighted avg       0.73      0.74      0.69       234\n",
      "\n",
      "(234,)\n",
      "(234,)\n",
      "----------------------------------------------------\n",
      "piano\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.90      0.95       413\n",
      "        True       0.96      1.00      0.98       879\n",
      "\n",
      "    accuracy                           0.97      1292\n",
      "   macro avg       0.98      0.95      0.96      1292\n",
      "weighted avg       0.97      0.97      0.97      1292\n",
      "\n",
      "True\n",
      "True\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.71      0.77        66\n",
      "        True       0.88      0.94      0.91       154\n",
      "\n",
      "    accuracy                           0.87       220\n",
      "   macro avg       0.86      0.83      0.84       220\n",
      "weighted avg       0.87      0.87      0.87       220\n",
      "\n",
      "(220,)\n",
      "(220,)\n",
      "----------------------------------------------------\n",
      "saxophone\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.89      0.94       904\n",
      "        True       0.90      0.99      0.94       846\n",
      "\n",
      "    accuracy                           0.94      1750\n",
      "   macro avg       0.94      0.94      0.94      1750\n",
      "weighted avg       0.95      0.94      0.94      1750\n",
      "\n",
      "True\n",
      "True\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.66      0.53      0.59       160\n",
      "        True       0.54      0.68      0.60       133\n",
      "\n",
      "    accuracy                           0.59       293\n",
      "   macro avg       0.60      0.60      0.59       293\n",
      "weighted avg       0.61      0.59      0.59       293\n",
      "\n",
      "(293,)\n",
      "(293,)\n",
      "----------------------------------------------------\n",
      "synthesizer\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.96      0.97       383\n",
      "        True       0.98      0.99      0.98       819\n",
      "\n",
      "    accuracy                           0.98      1202\n",
      "   macro avg       0.98      0.97      0.97      1202\n",
      "weighted avg       0.98      0.98      0.98      1202\n",
      "\n",
      "True\n",
      "True\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.35      0.19      0.24        59\n",
      "        True       0.71      0.86      0.78       140\n",
      "\n",
      "    accuracy                           0.66       199\n",
      "   macro avg       0.53      0.52      0.51       199\n",
      "weighted avg       0.61      0.66      0.62       199\n",
      "\n",
      "(199,)\n",
      "(199,)\n",
      "----------------------------------------------------\n",
      "trombone\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      1.00      0.90      1403\n",
      "        True       1.00      0.52      0.69       657\n",
      "\n",
      "    accuracy                           0.85      2060\n",
      "   macro avg       0.91      0.76      0.79      2060\n",
      "weighted avg       0.88      0.85      0.83      2060\n",
      "\n",
      "False\n",
      "False\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.71      0.99      0.83       243\n",
      "        True       0.75      0.06      0.11       105\n",
      "\n",
      "    accuracy                           0.71       348\n",
      "   macro avg       0.73      0.52      0.47       348\n",
      "weighted avg       0.72      0.71      0.61       348\n",
      "\n",
      "(348,)\n",
      "(348,)\n",
      "----------------------------------------------------\n",
      "trumpet\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      1.00      0.93      1351\n",
      "        True       1.00      0.78      0.87       849\n",
      "\n",
      "    accuracy                           0.91      2200\n",
      "   macro avg       0.94      0.89      0.90      2200\n",
      "weighted avg       0.92      0.91      0.91      2200\n",
      "\n",
      "False\n",
      "False\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.65      0.94      0.77       206\n",
      "        True       0.75      0.27      0.40       142\n",
      "\n",
      "    accuracy                           0.67       348\n",
      "   macro avg       0.70      0.61      0.59       348\n",
      "weighted avg       0.69      0.67      0.62       348\n",
      "\n",
      "(348,)\n",
      "(348,)\n",
      "----------------------------------------------------\n",
      "ukulele\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      1.00      0.89      1260\n",
      "        True       1.00      0.45      0.62       559\n",
      "\n",
      "    accuracy                           0.83      1819\n",
      "   macro avg       0.90      0.72      0.75      1819\n",
      "weighted avg       0.86      0.83      0.81      1819\n",
      "\n",
      "False\n",
      "False\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.70      1.00      0.82       219\n",
      "        True       1.00      0.02      0.04        95\n",
      "\n",
      "    accuracy                           0.70       314\n",
      "   macro avg       0.85      0.51      0.43       314\n",
      "weighted avg       0.79      0.70      0.59       314\n",
      "\n",
      "(314,)\n",
      "(314,)\n",
      "----------------------------------------------------\n",
      "violin\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.58      0.74       654\n",
      "        True       0.76      1.00      0.87       881\n",
      "\n",
      "    accuracy                           0.82      1535\n",
      "   macro avg       0.88      0.79      0.80      1535\n",
      "weighted avg       0.86      0.82      0.81      1535\n",
      "\n",
      "False\n",
      "True\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.57      0.30      0.39        88\n",
      "        True       0.66      0.86      0.75       141\n",
      "\n",
      "    accuracy                           0.64       229\n",
      "   macro avg       0.61      0.58      0.57       229\n",
      "weighted avg       0.62      0.64      0.61       229\n",
      "\n",
      "(229,)\n",
      "(229,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "voice\n",
      "\tTRAIN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.76      0.87       437\n",
      "        True       0.88      1.00      0.93       744\n",
      "\n",
      "    accuracy                           0.91      1181\n",
      "   macro avg       0.94      0.88      0.90      1181\n",
      "weighted avg       0.92      0.91      0.91      1181\n",
      "\n",
      "True\n",
      "True\n",
      "\tTEST\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.67      0.75        70\n",
      "        True       0.83      0.93      0.88       121\n",
      "\n",
      "    accuracy                           0.84       191\n",
      "   macro avg       0.84      0.80      0.82       191\n",
      "weighted avg       0.84      0.84      0.83       191\n",
      "\n",
      "(191,)\n",
      "(191,)\n"
     ]
    }
   ],
   "source": [
    "# This dictionary will include the classifiers for each model\n",
    "models = dict()\n",
    "\n",
    "# We'll iterate over all istrument classes, and fit a model for each one\n",
    "# After training, we'll print a classification report for each instrument\n",
    "for instrument in INSTRUMENTS:\n",
    "    \n",
    "    # Map the instrument name to its column number\n",
    "    inst_num = INSTRUMENTS[instrument]\n",
    "        \n",
    "    # Step 1: sub-sample the data\n",
    "    \n",
    "    # First, we need to select down to the data for which we have annotations\n",
    "    # This is what the mask arrays are for\n",
    "    train_inst = Y_mask_train[:, inst_num]\n",
    "    test_inst = Y_mask_test[:, inst_num]\n",
    "    \n",
    "    # Here, we're using the Y_mask_train array to slice out only the training examples\n",
    "    # for which we have annotations for the given class\n",
    "    X_train_inst = X_train[train_inst]\n",
    "    \n",
    "    # Step 3: simplify the data by averaging over time\n",
    "    \n",
    "    # Let's arrange the data for a sklearn Random Forest model \n",
    "    # Instead of having time-varying features, we'll summarize each track by its mean feature vector over time\n",
    "    X_train_inst_sklearn = np.mean(X_train_inst, axis=1)\n",
    "    \n",
    "    # Again, we slice the labels to the annotated examples\n",
    "    # We thresold the label likelihoods at 0.5 to get binary labels\n",
    "    Y_true_train_inst = Y_true_train[train_inst, inst_num] >= 0.5\n",
    "\n",
    "    \n",
    "    # Repeat the above slicing and dicing but for the test set\n",
    "    X_test_inst = X_test[test_inst]\n",
    "    X_test_inst_sklearn = np.mean(X_test_inst, axis=1)\n",
    "    Y_true_test_inst = Y_true_test[test_inst, inst_num] >= 0.5\n",
    "\n",
    "    # Step 3.\n",
    "    # Initialize a new classifier\n",
    "    clf = RandomForestClassifier(max_depth=8, n_estimators=100, random_state=0)\n",
    "    \n",
    "    # Step 4.\n",
    "    clf.fit(X_train_inst_sklearn, Y_true_train_inst)\n",
    "\n",
    "    # Step 5.\n",
    "    # Finally, we'll evaluate the model on both train and test\n",
    "    Y_pred_train = clf.predict(X_train_inst_sklearn)\n",
    "    Y_pred_test = clf.predict(X_test_inst_sklearn)\n",
    "    \n",
    "    print('-' * 52)\n",
    "    print(instrument)\n",
    "    print('\\tTRAIN')\n",
    "    print(classification_report(Y_true_train_inst, Y_pred_train))\n",
    "    print(Y_true_train_inst[3])\n",
    "    print(Y_pred_train[3])\n",
    "    print('\\tTEST')\n",
    "    print(classification_report(Y_true_test_inst, Y_pred_test))\n",
    "    \n",
    "    print(Y_true_test_inst.shape)\n",
    "    print(Y_pred_test.shape)\n",
    "    \n",
    "    # Store the classifier in our dictionary\n",
    "    models[instrument] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import plot, show, figure, imshow, xlim, ylim, title\n",
    "\n",
    "\n",
    "def plot_history():\n",
    "    plt.figure(figsize=(9,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train accuracy', 'Validation accuracy'], loc='upper left')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train loss', 'Validation loss'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "    # Step 3: simplify the data by averaging over time\n",
    "    # Instead of having time-varying features, we'll summarize each track by its mean feature vector over time\n",
    "    X_train_inst_sklearn = np.mean(X_train_inst, axis=1)\n",
    "    X_test_inst_sklearn = np.mean(X_test_inst, axis=1)\n",
    "    X_train_inst_sklearn = X_train_inst_sklearn.astype('float32')\n",
    "    X_train_inst_sklearn = lb.util.normalize(X_train_inst_sklearn)\n",
    "\"\"\"\n",
    "\n",
    "np.savez('models.npz',model=)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
